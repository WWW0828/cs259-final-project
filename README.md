# CS259 Chip Design Automation and Deep Learning Final Project
- UCLA, Fall 2024
- Instructors: Jason Cong, Yizhou Sun
- Contributors: Yu-Hsin Weng, Yi-Chun Lo

## Topic Description
This project explores advanced techniques for design space exploration (DSE) in high-level synthesis (HLS), aiming to enhance methods like exhaustive search, simulated annealing, and genetic algorithms used in GNN-DSE\cite{gnn-dse} and HARP\cite{harp2023}. Our goal is to demonstrate the effectiveness of our approach by applying it to 10 new programs, generated by modifying loop bounds in HLSyn examples or using entirely different ones. These programs will create a solution space of over 100,000 design points. With a search time limited to 4 hours, we aim to show a clear advantage of our method over the existing search techniques in HARP.
> See our [proposal](documentations/proposal.pdf) for more information.

## Technical Details
### 1. Reinforcement Learning
Train the Policy and Value Networks with
#### Policy Network
Given the current state (design), output the probability distribution of actions (action: inserting pragma p at loop l with factor f)
#### Value Network
Given the current state (design), output the estimated design quality (compute with some formulas using information such as valid or not, latency, resources usage)
#### Reward Function
Compute by the 2 pretrained models (classification, regression)
### 2. Monte Carlo Tree Search (MCTS)
#### Selection
Traverse the tree from the root to a leaf using a policy like UCT (Upper Confidence Bounds for Trees)
#### Expansion
Use the policy network to pick a best action for the leaf(state) selected in 1. Selection
#### Simulation
Simply use the value network to estimate the quality of the current state (after applying the best action from 2. Expansion)
#### Backpropogation
Update the UCT values of the node in the trajectory

## Implementation Plan
1. Implement Basic MCTS (without the use of policy and value networks), during MCTS, randomly pick an action in expansion phase
2. Integrate Policy and Value Network to MCTS. (Could be combined with 1 as NNs without training are simular to random agents)
3. Implement training pipeline (RL+MCTS) to optimize policy and value networks after MCTS.


## References
### HARP:  Robust GNN-based Representation Learning for HLS
- [github](https://github.com/UCLA-VAST/HARP), [paper](https://ieeexplore.ieee.org/document/10323853)

```
@inproceedings{harp2023,
    title={Robust GNN-based Representation Learning for HLS},
    author={Sohrabizadeh, Atefeh and Bai, Yunsheng and Sun, Yizhou and Cong, Jason},
    booktitle={In Proceedings of the 42nd IEEE/ACM International Conference on Computer-Aided Design (ICCAD)},
    year={2023}
}
```

### Towards a Comprehensive Benchmark for High-Level Synthesis Targeted to FPGAs
- [github](https://github.com/UCLA-DM/HLSyn), [paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/8dfc3a2720a4112243a285b98e0d4415-Paper-Datasets_and_Benchmarks.pdf)
```
@article{chang2023dr,
    title={Towards a Comprehensive Benchmark for High-Level Synthesis Targeted to FPGAs},
    author={Yunsheng Bai, Atefeh Sohrabizadeh, Zongyue Qin, Ziniu Hu, Yizhou Sun, and Jason Cong},
    journal={NeurIPS},
    year={2023}
}
```